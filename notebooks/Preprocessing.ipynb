{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pre-Processing**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that I have been working with so far was only 10% of the original dataset. In this notebook I will run the full dataset through the same process that I used on the 10% version. I believe that with GPU friendly models I will be able to run the full dataset through the training process. If not I can revert back to the 10% version. \n",
    "\n",
    "Since this is a competition dataset the data was already split into training data and test data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Set Directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and set the working directory for this project\n",
    "os.chdir(r'C:\\Users\\jdrel\\OneDrive\\Documents\\Data_Science\\Springboard\\Capstone-2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  **Load the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv('./data/raw/Full.data.corrected')\n",
    "\n",
    "# Load the columns of small_df\n",
    "small_df = pd.read_csv('./data/interim/Small_df.csv', nrows = 1)\n",
    "\n",
    "# Load columns of big_df\n",
    "big_df = pd.read_csv('./data/interim/Big_df.csv', nrows = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Process Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(dfx):\n",
    "    # I copy and pasted the column names from the website into this list\n",
    "    dfx.columns = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n",
    "                    \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\",\n",
    "                    \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "                    \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "                    \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\",\n",
    "                    \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\"srv_serror_rate\",\n",
    "                    \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "                    \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "                    \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \n",
    "                    \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"labels\"]\n",
    "    \n",
    "    # Between 'num_outbound_cmds', and 'is_host_login' there are 2 non zero values both are observations that were both normal.\n",
    "    # Use the drop methods to get rid of the near constants\n",
    "    dfx = dfx.drop(columns = ['num_outbound_cmds', 'is_host_login'], axis = 1)\n",
    "    '''\n",
    "    Now we separate the data into the feature variables (X) and the target variable (y). With the features dataframe we can use \n",
    "    the pd.get_dummies function to create dummy features for all the categories in the categorical columns. This will make it possible \n",
    "    to analyze all the features for multicolinearity and then use lasso regularization to determine the most important features.\n",
    "    '''\n",
    "    # Only use the X data so that it is easy to test for multicolinearity\n",
    "    X_data = dfx.drop('labels', axis = 1)\n",
    "\n",
    "    # Find the categorical columns that need to be made numerical for analysis\n",
    "    cat_cols = list(X_data.select_dtypes(include = 'object').columns)\n",
    "\n",
    "    # Creating a completely numerical dataset that is usable for analysis\n",
    "    x_num_data = pd.get_dummies(X_data, columns = cat_cols, \n",
    "                            # When testing for multi-co-linearity it is important to drop one of the dummies\n",
    "                            # so that that column doesn't get flagged\n",
    "                                drop_first = True)\n",
    "    \n",
    "    '''\n",
    "    Skewed features are a problem as they make it hard for models to accurately describe interactions between different features because\n",
    "    the arbitrary size of some feature will completely warp the math. To fix this we can scale the features with StandardScaler so as to\n",
    "    preserve the nature of the feature without destroying the model by its size.\n",
    "    '''\n",
    "    # Create the scaler object\n",
    "    scaler = StandardScaler()\n",
    "    # fit the scaler to the dataset\n",
    "    scaler.fit(x_num_data)\n",
    "    # Scale the dataset\n",
    "    x_num_data_scaled = pd.DataFrame(scaler.transform(x_num_data), columns = x_num_data.columns)\n",
    "\n",
    "    '''\n",
    "    Creating the features 'Syn Error' and 'Rej Error'\n",
    "    '''\n",
    "    # Define the Syn Error columns\n",
    "    serror = ['serror_rate', 'srv_serror_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate']\n",
    "\n",
    "    # Define the Syn Error columns\n",
    "    rerror = ['rerror_rate', 'srv_rerror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
    "\n",
    "    # Drop the redundant columns\n",
    "    df = x_num_data_scaled.drop(columns = [*serror, *rerror])\n",
    "\n",
    "    # Create new Syn Error column as mean of the former Syn Error columns\n",
    "    df['Syn Error'] = x_num_data_scaled[serror].max(axis = 1)\n",
    "\n",
    "\n",
    "    # Create new Syn Error column as mean of the former Syn Error columns\n",
    "    df['Rej Error'] = x_num_data_scaled[rerror].max(axis = 1)\n",
    "\n",
    "    '''\n",
    "    get the list of the columns in big and small dataframe\n",
    "    '''\n",
    "    # Create the list of all the features that were in small_df\n",
    "    small_features = list(small_df.columns)\n",
    "    # Drop the target column 'labels' from that list\n",
    "    small_features.remove('labels')\n",
    "    # Create the list of all the features that were in big_df\n",
    "    big_features = list(big_df.columns)\n",
    "    # Drop the target column 'labels' from that list\n",
    "    big_features.remove('labels')\n",
    "\n",
    "    '''\n",
    "    Now it is time to create the dataset\n",
    "    '''\n",
    "    #Store as a global variable since it is the end product\n",
    "    global X_small\n",
    "    # Generate the training dataframe for features that determine intrusion\n",
    "    X_small = df[small_features]\n",
    "    #Store as a global variable since it is the end product\n",
    "    global X_big\n",
    "    # Generate the training dataframe for features that determine type of intrusion\n",
    "    X_big = df[big_features]\n",
    "    #Store as a global variable since it is the end product\n",
    "    global y\n",
    "    # Generate y_train\n",
    "    y = data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
