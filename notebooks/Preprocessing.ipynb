{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>**Pre-Processing**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the requirements for this notebook have been fulfilled in the EDA. I will copy and paste over the code that I used in EDA for those parts. But first we must import packages and import the data.\n",
    "\n",
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and set the working directory for this project\n",
    "os.chdir(r'C:\\Users\\jdrel\\OneDrive\\Documents\\Data_Science\\Springboard\\Capstone-2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>tcp</th>\n",
       "      <th>http</th>\n",
       "      <th>SF</th>\n",
       "      <th>215</th>\n",
       "      <th>45076</th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.2</th>\n",
       "      <th>0.3</th>\n",
       "      <th>0.4</th>\n",
       "      <th>...</th>\n",
       "      <th>0.17</th>\n",
       "      <th>0.00.6</th>\n",
       "      <th>0.00.7</th>\n",
       "      <th>0.00.8</th>\n",
       "      <th>0.00.9</th>\n",
       "      <th>0.00.10</th>\n",
       "      <th>0.00.11</th>\n",
       "      <th>0.00.12</th>\n",
       "      <th>0.00.13</th>\n",
       "      <th>normal.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>162</td>\n",
       "      <td>4528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>236</td>\n",
       "      <td>1228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>233</td>\n",
       "      <td>2032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>239</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>238</td>\n",
       "      <td>1282</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  tcp  http  SF  215  45076  0.1  0.2  0.3  0.4  ...  0.17  0.00.6  \\\n",
       "0  0  tcp  http  SF  162   4528    0    0    0    0  ...     1     1.0   \n",
       "1  0  tcp  http  SF  236   1228    0    0    0    0  ...     2     1.0   \n",
       "2  0  tcp  http  SF  233   2032    0    0    0    0  ...     3     1.0   \n",
       "3  0  tcp  http  SF  239    486    0    0    0    0  ...     4     1.0   \n",
       "4  0  tcp  http  SF  238   1282    0    0    0    0  ...     5     1.0   \n",
       "\n",
       "   0.00.7  0.00.8  0.00.9  0.00.10  0.00.11  0.00.12  0.00.13  normal.  \n",
       "0     0.0    1.00     0.0      0.0      0.0      0.0      0.0  normal.  \n",
       "1     0.0    0.50     0.0      0.0      0.0      0.0      0.0  normal.  \n",
       "2     0.0    0.33     0.0      0.0      0.0      0.0      0.0  normal.  \n",
       "3     0.0    0.25     0.0      0.0      0.0      0.0      0.0  normal.  \n",
       "4     0.0    0.20     0.0      0.0      0.0      0.0      0.0  normal.  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "data = pd.read_csv('./data/raw/Full.data.corrected')\n",
    "# Look at the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I copy and pasted the column names from the website into this list\n",
    "data.columns = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n",
    "                \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\",\n",
    "                \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "                \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "                \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\",\n",
    "                \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\"srv_serror_rate\",\n",
    "                \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "                \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "                \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \n",
    "                \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"labels\"]\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between 'num_outbound_cmds', and 'is_host_login' there are 2 non zero values both are observations that had no intrusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4898430\n",
       "Name: num_outbound_cmds, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['num_outbound_cmds'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal.    2\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['is_host_login'] == 1, 'labels'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both variables add no value I will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the drop methods to get rid of the constants\n",
    "data = data.drop(columns = ['num_outbound_cmds', 'is_host_login'], axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we separate the data into the feature variables (X) and the target variable (y). With the features dataframe we can use the pd.get_dummies function to create dummy features for all the categories in the categorical columns. This will make it possible to analyze all the features for multicolinearity and then use lasso regularization to determine the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use the X data so that it is easy to test for multicolinearity\n",
    "X_data = data.drop('labels', axis = 1)\n",
    "\n",
    "# Find the categorical columns that need to be made numerical for analysis\n",
    "cat_cols = list(X_data.select_dtypes(include = 'object').columns)\n",
    "\n",
    "# Creating a completely numerical dataset that is usable for analysis\n",
    "x_num_data = pd.get_dummies(X_data, columns = cat_cols, \n",
    "                        # When testing for multi-co-linearity it is important to drop one of the dummies\n",
    "                        # so that that column doesn't get flagged\n",
    "                            drop_first = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skewed Features**\n",
    "\n",
    "Skewed features are a problem as they make it hard for models to accurately describe interactions between different features because the arbitrary size of some feature will completely warp the math. To fix this we can scale the features with StandardScaler so as to preserve the nature of the feature without destroying the model by its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scaler object\n",
    "scaler = StandardScaler()\n",
    "# fit the scaler to the dataset\n",
    "scaler.fit(x_num_data)\n",
    "# Scale the dataset\n",
    "x_num_data_scaled = pd.DataFrame(scaler.transform(x_num_data), columns = x_num_data.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all of the features that have syn error and rej error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Syn Error columns\n",
    "serror = ['serror_rate', 'srv_serror_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate']\n",
    "\n",
    "# Define the Syn Error columns\n",
    "rerror = ['rerror_rate', 'srv_rerror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
    "\n",
    "# Drop the redundant columns\n",
    "df = x_num_data_scaled.drop(columns = [*serror, *rerror])\n",
    "\n",
    "# Create new Syn Error column as mean of the former Syn Error columns\n",
    "df['Syn Error'] = x_num_data_scaled[serror].max(axis = 1)\n",
    "\n",
    "\n",
    "# Create new Syn Error column as mean of the former Syn Error columns\n",
    "df['Rej Error'] = x_num_data_scaled[rerror].max(axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop These columns that have no clear connection but are multi-colinear\n",
    "df = df.drop(['srv_count','service_ecr_i', 'dst_host_same_src_port_rate'],axis = 1)\n",
    "\n",
    "# Drop the non-rate column\n",
    "df = df.drop('dst_host_srv_count', axis = 1)\n",
    "\n",
    "# Create the srv_rate column\n",
    "srvrate = ['dst_host_same_srv_rate', 'same_srv_rate']\n",
    "\n",
    "# Define srvrate\n",
    "df['srv_rate'] = df[srvrate].max(axis = 1)\n",
    "\n",
    "# Drop the srvrate columns\n",
    "df = df.drop(srvrate, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [['wrong_fragment', 'hot', 'count', 'srv_diff_host_rate', 'dst_host_count', 'protocol_type_udp',\n",
    "            'service_eco_i', 'service_ftp_data', 'service_smtp', 'flag_RSTR', 'Syn Error', 'Rej Error']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the amount of time that it took to run the eda for the bigger dataset I have made the decision to only use the features from the smaller df. Projects like this always have more work to do but my computer doesn't have enough power to analyze the larger dataset in a reasonable time frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training dataframe with the non zero lasso coefficients\n",
    "X_train = df[features]\n",
    "# Take a look at the dataset\n",
    "X_train.head()\n",
    "# Generate y_train\n",
    "y_train = data['labels']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
